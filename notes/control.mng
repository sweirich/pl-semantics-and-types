\chapter{Control Effects}
\label{ch:control}

Many languages include features for throwing and catching exceptions.
Exceptions provide a way for programs to signal errors or break out of loops
early.

Fewer languages provide first-class access to the control stack through
control operators. Scheme (and its modern-day variant Racket), includes
\texttt{call-with-current-continuation}, or \texttt{call-cc}. (Other languages
that have included this feature are Standard ML of New Jersey, Ruby, and
Smalltalk.)  This feature acts a little like exceptions --- by passing a
captured continuation, users can go back to earlier parts of the stack --- but
it is much more powerful.  Captured continuations are first class. They can be
saved in data structures and stored in function closures. That means that
invoking a continuation may take your program to a stack that is very
different than the current one.

First-class continuations can be used to implement other high-level control
structures that are difficult to implement cleanly otherwise, such as exceptions,
coroutines or generators, or non-deterministic backtracking. Furthermore, even
if languages do not include support for first-class continuations, they can be
implemented through the use of ``continuation-passing-style'' or a concurrency
monad.

Effect handlers are newer still and have only recently been added to the OCaml
language. This feature, based on an older idea of \emph{delimited
continuations} combines features of first-class continuations and exception
handling to provide more compositionality and more efficient implementation.
This language feature is being actively explored in research languages such as
Koka and Eff, and was briefly included in the Scala language.

In this chapter we will consider how to extend the semantics of our stack
based language with new forms of control operations. 

\section{Exceptions}

Most languages include an extensible variant type for exceptions. For
convenience, this type \texttt{exn} or \texttt{Exception} can be extended with
new options corresponding to various forms of errors (\texttt{Failure} or
\texttt{NullPointerException}) and signals (\texttt{Not\_found} or
\texttt{NoSuchElementException}).  For simplicity in our formalism, we will
define a new type $[[Exn]]$ as our exception type, but this type will be
isomorphic to natural numbers so that we have as many exceptions as we need
and can distinguish between different exception values.

Our extension with exceptions includes two new terms in the language: a way to
\emph{raise} or trigger an exception, and a way to install an exception
handler with \texttt{try}.

\[
\begin{array}{llcl}
\mathit{term} & [[e]] &::=& [[try e1 with exn x => e2]]\ |\ [[raise v]] \\
\mathit{frame}& [[f]] &::=& [[try _  with exn x => e2]] \\
\end{array}
\]

There are three new small-step rules that work with exceptions. 
The \rref{ssm-try} pushes an exception handler frame onto the stack 
and continues executing the body of the ``try''. The \rref{ssm-discard}
pops this exception handler off the stack when the body returns a value. 
Finally, the \rref{ssm-raise} uses an auxiliary function (shown below) 
to scan the stack for a current exception handler when an exception is 
raised.

\drules{$[[m |-> m']]$}{Stack-based small-step rules}
{ssm-try,ssm-discard,ssm-raise}
\[
\begin{array}{ll}
[[find_exn (try _ with exn x => e2 : s) v]] &= [[<s, e2[v/x]>]]\\
[[find_exn (frame : s) v]] &= [[find_exn s v]] \\
[[find_exn nil v]] & = [[< nil , raise v >]] \\
\end{array}
\]

When $[[find_exn s v]]$ successfully finds an exception handler frame on the
stack (see first line of the definition) then the machine switches to the
stack immediately below the handler and the code in the handler. The value
$[[v]]$ is the thrown exception value (a natural number in this case) and is
substituted for $[[x]]$ in the handler code.  If the $[[find_exn s v]]$ reaches
the end of the stack without finding an exception handler, the machine stops
with an empty stack and an uncaught exception (signified by the $[[raise v]]$ term).
This term cannot step because the \rref{ssm-rule} applies only when the stack is not
empty.

\subsection{Type Safety in the presence of exceptions}

The typing rule for the two new terms are shown below:

\drules[tv]{$[[G ||- v : tau]]$}{Typing rules for values}
{exn}
\drules[te]{$[[G ||- e : tau]]$}{Typing rules for terms}
{raise,try}

Given any natural number, the value $[[exn k]]$ has the type $[[Exn]]$.  Then
the term $[[raise v]]$ expects its argument to be an exception value. Note
that this term could have any type, including $[[Void]]$.  The $[[raise v]]$
term never produces a value---it just jumps to some other part of the
program. So it is safe to pretend that this term produces any type of value
that we want.

The term $[[try e1 with exn x => e2]]$ requires both the body of the try block
$[[e1]]$ and the exception handler $[[e2]]$ to have the same type. This
expression will either terminate normally, or it will trigger the handler. We
can't tell which will happen statically, so the type system requires them to
be the same, just as in pattern matching expressions. The variable $[[x]]$ is
a name for the exception value that was thrown.

We can prove type safety for our stack-based language, but we need to extend
our type system to be able to talk about stacks, frames, and machines. All
three of these judgments are for runtime constructs, so we only care about
type checking them with the empty context.

\begin{definition}[Frame, Stack and Machine typing] \
\drules[tf]{$[[||- frame : tau1 ~> tau2 ]]$}{Frame $[[frame]]$ takes values of type $[[tau1]]$ to stacks of type $[[tau2]]$}
{let,try}
\drules[tss]{$[[||- s : tau1]]$}{Stack $s$ accepts terms of type $[[tau1]]$}
{nil,cons}
\drules[m]{$[[||- m ok]]$}{Machine $m$ is well-typed}{ok-s}
\end{definition}

We expect our machines to stop executing when the stack is empty and the term
has returned a value, or when the stack is empty and the term has raised an
exception (which is uncaught).
\begin{definition}[Terminal machines]
A machine is in a terminal state if it is of the form $[[<nil, ret v>]]$ or $[[<nil, raise v>]]$. 
\end{definition}

We can use the definition of terminal machine to talk about what it means for computation to not get stuck. 
\begin{theorem}[Type Safety]
If $[[||- m ok]]$ and $[[ m |->* m' ]]$ then either $[[m']]$ is a terminal machine or there exists some $[[m'']]$ such that $[[ m' |-> m'']]$.
\end{theorem}
The proof of this property is via the preservation and progress
theorems. There are a few observations to make about this proof.

First, note that the \rref{ssm-raise} case of the preservation theorem
requires showing that looking for an exception handler produces an well-typed
machine. It doesn't matter what type of stack we have when we raise the
exception; if its well-formed, then any exception handler that we find on the
stack will produce a well-formed machine.
\begin{lemma}[Preservation for $\ottkw{find\_exn}$]
If $[[||- v : Exn]]$ and $[[ ||- s : tau1]]$, then $[[ ||- find_exn s v ok ]]$
\end{lemma}

Second, notice more generally that we don't (need to) track the machine type in the 
$[[||- m ok]]$ judgment. This means that our preservation theorem doesn't necessarily 
``preserve'' the type of the machine, it just preserves the fact that the machine is 
well-formed as it steps.
\begin{lemma}[Preservation]
If $[[||- m ok]]$ and $[[m |-> m']]$ then $[[ ||- m ok ]]$.
\end{lemma}

Third, note our type safety theorem does not rule out uncaught exceptions. The
machine configuration $[[ <nil, raise v>]]$ results from throwing an exception
when there is no handler on the stack, and is considered a well-formed
terminal machine.

It would be difficult for our type system to do better. If we want to be sure
that all exceptions are caught by appropriate handlers, i.e. that our language
is \emph{exception safe} in addition to \emph{type safe}, we might use some
sort of type-and-effect system that tracks which exceptions are thrown and
removes those effects when a try-block is in play. The Java Language includes
this capability with its mechanism for checked exceptions. However, this
feature has turned out to not be very useful to developers.

Without exception safety, type safety is a weaker theorem. Instead of saying
that a well-typed program either diverges or returns a value, we say now that
a well-typed program either diverges, returns a value, or throws an
exception. More things could happen. What this means is that if we have a
language that is not type-safe because it gets stuck, we can trivially make it
type-safe by changing the semantics to throw an exception in all places where
it would get stuck. (This is a bit better than changing the semantics to go
into an infinite loop in all places where it would get stuck, an option that
was already available to us.) Our type safety theorem is meaningful, but we
have to realize that its utility depends on the language semantics (as
always). We need to make sure that there are enough reduction rules to
actually compute the answers that we need. A trivial language that always and
immediately raises an exception would also be type safe.

\section{Continuations}

Control operations, such as \texttt{call/cc}, reify the control stack 
as a data structure. This provides a more expressive way to ``jump'' to 
new computation contexts. 


\begin{definition}[Control operations]
\[
\begin{array}{llcl}
\mathit{types}& [[tau]] &::=& [[Cont tau]] \\
\mathit{value}& [[v]] &::=& [[cont s]] \\
\mathit{term} & [[e]] &::=& [[letcc x in e]]\ |\ [[throw v1 v2]] \\
\end{array}
\]
\end{definition}

Let's add a $[[letcc x in e]]$ operation to the \rec\ language. This term
captures the current stack as a new continuation value $[[cont s]]$, bound to
the variable $[[x]]$. In the scope of $[[x]]$, the $[[throw v1 v2]]$ operation
changes whatever the current execution stack is to this saved stack, which
also receives the value $[[v2]]$.

\begin{definition}[Small-step stack based semantics for control] \ \\
\drules{$[[m |-> m']]$}{Stack-based small-step rules}
{ssm-letcc,ssm-throw}
\end{definition}

Because we have added a new form of value to the language, we also add a new
type, called $[[Cont tau]]$. The $[[cont s]]$ expression introduces a new
continuation value, formed from a stack of type $[[tau]]$. In a $[[letcc x in e]]$
term, $[[x]]$ has a continuation type that must be the same as the type of the 
whole term; this is the point of the computation that we will come back to when the 
continuation is invoked.  The $\ottkw{throw}$ term takes any continuation and 
any value

\drules[tv]{$[[G ||- v : tau]]$}{Value typing}
{cont}
\drules[te]{$[[G ||- e : tau]]$}{Term typing}
{letcc,throw}

\subsection{Continuations and the Curry-Howard Isomorphism}

The Curry-Howard Isomorphism relates constructive logic with typed functional
programming. It observes that there is a correspondence between logical
propositions and types, and with the proofs of those propositions and programs
that have those types. This idea is the basis of modern proof assistants based
on dependent type theory, such as Rocq, Lean or Agda.  To show that a
proposition is true, you just need to find a program that has that type.

\begin{definition}[Correspondence between propositions and types]
\[
\begin{array}{ll}
A \wedge B & [[ tau1 * tau2 ]] \\
A \vee B & [[ tau1 + tau2 ]] \\
A \Rightarrow B & [[ tau1 -> tau2 ]] \\   
\neg A & [[ A -> Void ]] \\
\textit{True}  &  [[ Unit ]] \\
\textit{False} &  [[ Void ]] \\
\end{array}
\]
\end{definition}

This chart shows the correspondence between common logical combinators
(conjunction, disjunction, implication, negation) and types. And, it makes sense that constructive proofs of these combinators correspond to programs. For example, a constructive proof of a conjunction $A \wedge B$ is a pair of proofs, one which shows $A$ and another that shows $B$. We can see this in the definition of conjunction in Rocq standard library:

\begin{lstlisting}
Inductive and (A B : Prop) : Prop := 
    conj : A -> B -> A /\ B.
\end{lstlisting}

Similarly, a proof of a disjunction $A \vee B$ is either a proof of $A$, or a proof of $B$. This corresponds to a sum type.

\begin{lstlisting}
Inductive or (A B : Prop) : Prop :=  
    or_introl : A -> A \/ B 
  | or_intror : B -> A \/ B.
\end{lstlisting}

The proof of an implication $A \Rightarrow B$ is a function that takes a proof
of $A$ to a proof of $B$. This corresponds to a function.  In constructive
logic, negation $\neg A$ is the same as saying that $A$ is impossible. In
other words, it is a function that takes a proof of $A$ to a proof of
$\textit{False}$---i.e. a type that has no inhabitants (so there can never be
a proof of $\textit{False}$).

For example, we can prove various properties by creating programs in Gallina, the
language of the Rocq proof assistant. (Usually we don't write these programs directly; tactics let us construct them more conveniently.) For example, we can show (half)
of one of the de Morgan laws as follows:
\begin{lstlisting}
Definition demorgan1 : not P /\ not Q -> not (P \/ Q) := 
  fun npnq => match npnq with 
             | conj np nq => 
                fun p_or_q => 
                  match p_or_q with 
                  | or_introl p => np p
                  | or_intror q => nq q
                  end
             end.
\end{lstlisting}
The inverse of this law is also expressible using a proof term in Rocq.
\begin{lstlisting}
Definition demorgan2 : not (P \/ Q) -> not P /\ not Q := 
   fun h => conj (fun p => h (or_introl p)) (fun q => h (or_intror q)).
\end{lstlisting}

However, there is a difficulty with the other de Morgan identity. One direction 
is also easy. 
\begin{lstlisting}
Definition demorgan3: not P \/ not Q -> not (P /\ Q) := 
   fun h => match h with 
            | or_introl np => fun pq => match pq with 
                                        | conj p q => np p
                                        end
            | or_intror nq => fun pq => match pq with 
                                        | conj p q => pq q
                                        end
            end.
\end{lstlisting}

However, the remaining direction is not constructively provable.
\begin{lstlisting}
Lemma demorgan4 : not (P /\ Q) -> not P \/ not Q.
\end{lstlisting}
There is no way to prove this proposition in Rocq, without using 
any axioms. 

What does this have to do with control operators?  Suppose Rocq had explicit
control over continuations. We can pretend that is the case by assuming that
there is some continuation type $\ottkw{cont}$, and the two operations
$\ottkw{callcc}$ and $\ottkw{throw}$. (To make it easier to add the control
operator to Rocq, we use $\ottkw{callcc}$ instead of $\ottkw{letcc}$. The
former uses a function argument instead of a let expression to bind the
current continuation.)
 
Furthermore, we can give reasonable definitions to the continuation type and
the throw operation. All we \emph{really} need to axiomatize is callcc.
\begin{lstlisting}
Definition cont  : Prop -> Prop := fun A => False.
Definition throw : forall A, cont A -> A -> False := fun A k v => k v.
Axiom callcc     : forall A, ((cont A) -> A) -> A.
\end{lstlisting}

If you look closely at the type of $\ottkw{callcc}$, you may recognize it as
Peirce's law, usually written $((P \Rightarrow Q) \Rightarrow P) \Rightarrow
P$. This axiom is equivalent to the law of the excluded middle, which states
that any proposition is either true or false, $P \vee \neg P$.  Adding either
Peirce's law or the law of the excluded middle to constructive logic turns it
into classical logic. That means that these axioms are consistent with
constructive logic, but cannot be proven constructively. If you would like to
reason using classical logic in Rocq, you need to assume one of these axioms.

These axioms are equivalent. For example, you can derive the law of the excluded 
middle using callcc.
\begin{lstlisting}
Definition excluded_middle {A} : A \/ cont A :=
  callcc (fun k => or_intror (fun a => 
             throw k (or_introl a))).
\end{lstlisting}
This proof captures the current continuation $k$ and then immediately asserts
that $A$ is not true. But if it was the case that $A$ was actually true
(i.e. someone called this function with some $a$ that is a proof of $A$), then
we can throw this proof to the captured continuation to assert that $A$ was
really true all along. It's a bit like time travel---we can assert whatever we want 
without evidence, but if we ever find out that we were wrong, we can go back in 
time and replace our answer with the other version.

Furthermore, with $\ottkw{callcc}$, we can show the problematic de Morgan law from above: 
\begin{lstlisting}
Definition demorgan4 : not (P /\ Q) -> not P \/ not Q := 
  fun npq => callcc (fun k => 
     or_introl (fun p => throw k (or_intror (fun q => npq (conj p q))))). 
\end{lstlisting}
What does this proof mean? To show that $not (P /\ Q)$ implies $not P \/ not Q$,
we take a proof that both $P$ and $Q$ cannot be true simultaneously and then
capture the current continuation $k$. At this point, we assert that it is $P$
that is not true, by showing a proof that $P$ leads to
$\textit{False}$. However, if we ever receive a proof of $P$, we use the
captured continuation to go back to where we started and assert that no, it
was $q$ that was not true. If, in this case, we also receive a proof of $Q$,
we can use our assumption with the evidence for $P$ and $Q$ that we have
already received.

Another proposition that is equivalent to Peirce's law and the law of the excluded middle is double negation elimination. 

\[ not (not P) \Rightarrow P \]

This proposition holds classically, but not constructively. In other words, $P$ in classical logic $P$ is the same as its double negation, but they are different propositions in constructive logic.  

In fact, even though a property $P$ may only be provable using classical
logic, we can prove its double negation, i.e. $not (not P)$, using
constructive logic. To show this fact, we need a technique that allows us to
transform a proof term that uses $\ottkw{callcc}$ into one that does not.
And that technique is called \emph{Continuation-Passing-Style conversion} or CPS conversion.

\subsection{CPS conversion}

CPS conversion is a translation from a language that includes explicit
continuation operations, like $\ottkw{letcc}$ or $\ottkw{callcc}$ into one
that does not include these constructs. It is useful for logicians, but it is
more useful for programmers, providing access to continuation-based
programming even when it is not supported by the underlying programming
language.

The key idea of CPS is that we will translate the program so that every
function will take a second argument: its continuation. Furthermore, once the
function has computed its result it will call this continuation with that
result instead of returning it. Functions never return when written in
continuation-passing-style, they only jump to the next instruction.  That
means that in order to call a function, we need to give it not just its
argument, but what it should do next---its continuation---sort of like 
putting a return address on the stack when we call a function.

Because the continuation is always available for every term, it is easy to
implement $[[letcc x in e]]$; we only need to bind $x$ to that continuation.

Our goal is to define a translations for terms $[[E{e}w]]$ and for types $[[T{tau}]]$, such that the following theorem holds. 
\begin{theorem}[\link{control/cps.v}{top_level}{CPS translation is type preserving}]\ \\
If $[[ ||- e : tau ]]$ then $[[ ||- \x. E{e} x : (T{tau} -> Void) -> Void ]]$
\end{theorem}
The term translation $[[E{e}w]]$ takes two arguments: a term to translate $e$
and a continuation $w$. This continuation should have type $[[T{tau} ->
    Void]]$, so that the type of the top-level translation is the double
negation (plus inner type translation) of the original type of the term.

The type translation mostly leaves the structure of the types alone. However,
it makes two changes. For function types, instead of returning a result of
type $[[T{tau2}]]$, the function is transformed to take a continuation
argument instead.  The type of the continuation argument is $[[C{tau2}]]$. The
function doesn't return a type anymore (so its result type is $[[Void]]$),
instead it passes its result to this continuation.  We also use $[[C{tau}]]$
as the translation of the continuation type.

\begin{definition}[\link{control/cps.v}{transTy}{Type translation}]
\[
\begin{array}{lcl}
[[ T{ Nat }          ]] &=& [[ Nat ]]\\
[[ T{ Void }         ]] &=& [[ Void ]]\\
[[ T{ tau1 * tau2 }  ]] &=& [[ T{ tau1 } * T{ tau2 } ]]\\
[[ T{ tau1 + tau2 }  ]] &=& [[ T{ tau1 } + T{ tau2 } ]]\\
[[ T{ alpha }        ]] &=& [[ alpha ]] \\
[[ T{ mu alpha. tau }]] &=& [[ mu alpha. T{ tau } ]] \\
[[ T{ tau1 -> tau2 } ]] &=& [[ T{ tau1 } -> C{ tau2 } -> Void ]]\\
[[ T{ Cont tau }     ]] &=& [[ C{ tau } ]] \\
\\
[[ C{ tau } ]]          &=& [[ T{ tau } -> Void ]] \\
\end{array}
\]
\end{definition}




A continuation $[[C{tau}]]$ is a function that wants an argument of type
$[[T{tau}]]$. But what should its result type be? If we want the theorem above
to hold, then it has to be $[[Void]]$. So continuations themselves can never
return any answers.

Wait a minute. If continuations never return, and we translate $[[e]]$ to
$[[\x.E{e} x]]$, i.e. a term waiting for its continuation, what continuation
do we pass in for $[[x]]$ to run the translated term? How do we get the
result? Continuations cannot return results, so they need to communicate them
in another way. Therefore, we add the $[[exit v]]$ construct to the
language. This term aborts the computation immediately. It doesn't return $v$,
it \emph{exits} with $v$, where an exited machine is a new terminal state.

\[ \drule{ssm-exit} \qquad \drule{te-exit} \]

It may seem like a subtle difference between returning a value and exiting
with a value, but the payoff is in the typing rule. A returned value may be
used by another computation.  So it's type needs to be the same as the type of
the value. But an exited value is \emph{done}. No one can ever use it. So the
typing rule for exit can have any type, including $[[Void]]$.  That means that
to run the translated term, we can pass in $[[\y. exit y]]$ as the initial
continuation.

The translation for terms $[[E{ e } w]]$ is defined mutually with a
translation for values $[[V{v}]]$ and stacks $[[S{s}]]$. We use the
metavariable $w$ for values used as continuations by the term
translation. (The more common practice is to use a $k$, but already use that
letter to stand for for natural numbers.)
\begin{definition}[\link{control/cps.v}{transTm}{Term, Value and Stack translation}]
\[
\begin{array}{lcl}
[[ V{ x }            ]] &=& [[ x ]] \\
[[ V{ zero }         ]] &=& [[ zero ]] \\
[[ V{ succ v }       ]] &=& [[ succ V{ v } ]] \\
[[ V{ unit }         ]] &=& [[ unit ]] \\
[[ V{ \x . e }       ]] &=& [[ \ x . ret \ y. E{ e } y ]] \\
[[ V{ ( v1 , v2 ) }  ]] &=& [[ ( V{ v1 } , V{ v2 } ) ]] \\
[[ V{ inj1 v }       ]] &=& [[ inj1 V{ v } ]] \\
[[ V{ inj2 v }       ]] &=& [[ inj2 V{ v } ]] \\
[[ V{ fold v }       ]] &=& [[ fold V{ v } ]] \\
[[ V{ rec x . v }    ]] &=& [[ rec x.  V{ v }  ]] \\
[[ V{ cont s }       ]] &=& [[ S{ s } ]] \\
\\
[[ E{ ret v } w      ]] &=& [[ w V{ v } ]] \\ 
[[ E{ let x = e1 in e2 } w ]] &=& [[ E{ e1 } (\x. E{ e2 } w ) ]] \\
[[ E{ v1 v2 } w      ]] &=& [[ let x = V{ v1 } V{ v2 } in x w ]] \\
[[ E{ letcc x in e } w ]] &=& [[ let x = ret w in E{ e }w ]] \\ 
[[ E{ throw v1 v2 } w ]]&=& [[ V{ v1 } V{ v2 } ]] \\
[[ E{ exit v } w     ]] &=& [[ exit V{v} ]] \\
[[ E{ prj1 v } w     ]] &=& [[ let x = prj1 V{ v } in w x ]] \\
[[ E{ prj2 v } w     ]] &=& [[ let x = prj2 V{ v } in w x ]] \\
[[ E{ unfold v } w   ]] &=& [[ let x = unfold V{ v } in w x ]] \\
[[ E{ case v of { 0 => e1 ; S y => e2 } } w ]] 
  &=& [[ case V{ v } of { 0 => E{ e1 } w ; S y => E{ e2 } w } ]] \\
[[ E{ case v of { inj1 x => e1 ; inj2 y => e2 } } w ]] 
  &=& [[ case V{ v } of { inj1 x => E{ e1 } w ; inj2 y => E{ e2 } w } ]] \\
\\
[[ S{ nil }         ]] &=& [[ \x. exit x ]] \\
[[ S{ let x = _ in e2 : s } ]] &=& [[ \x. E{ e2 } S{ s } ]] \\
\end{array}
\]
\end{definition}
The value translation produces a new value through structural recursion. For
function values, the translation introduces a second abstraction (for the
continuation $y$ that will be passed to the function). This continuation is
used to translate the body of the function. For continuation values, $[[cont
    s]]$, the translation uses the \emph{stack translation} to convert an
explicit stack into a continuation function. If the stack is empty, then the
continuation function should exit with its value. Otherwise, if there is a
frame on the stack, the continuation function uses the term translation to
process that value, with the continuation computed from the rest of the stack.

The term translation is more interesting. Instead of returning a value, the
translation passes it to the continuation $w$. Let expressions use the
continuation argument for sequencing. In an application, the translated
function expects a continuation argument in addition to the usual one. After
the first (partial) application, the result is applied to the current
continuation.  For $\ottkw{letcc}$, the current continuation value is named by
a let expression before continuing the translation.  For $\ottkw{throw}$, the
continuation value is applied to the argument, the current continuation is
discarded.  Similarly $\ottkw{exit}$, discards the current continuation and
just halts the program.  The remainder of the cases merely propagate the
continuation through the translation.

Finally, we need to define the translation of substitutions. We translate
substitution by translating each of the types or values in the range of the
substitution.

\begin{definition}[Substitution translation]
\[ [[ T{ sigma } alpha ]] = [[ T{ sigma alpha } ]]  \]
\[ [[ V{ sigma } x ]] = [[ V{ sigma x } ]]  \] 
\end{definition}

\paragraph{Properties of the type translation}

The type translation commutes with renaming and substitution.
\begin{lemma}[\link{control/cps.v}{transTy_ren}{Type translation renaming}]
\[ [[ T{ tau }<xi> ]] = [[T{ tau<xi> } ]] \]
\end{lemma}
\begin{lemma}[\link{control/cps.v}{transTy_subst}{Type translation substitution}]
\[ [[ T{ tau }[ T{ sigma } ] ]] = [[T{ tau[sigma] } ]] \]
\end{lemma}

This final property establishes the result that we need for recursive types:

\begin{corollary}[\link{control/cps.v}{transTy_Mu}{Recursive type translation}]
\[ [[ T{ tau }[ mu alpha. T{ tau } / alpha ] ]] = [[ T{ tau [mu alpha. tau/alpha] } ]] \]
\end{corollary}

\paragraph{Properties of the term and value translations}

To prove our main lemma, we need to generalize it to talk about open
terms. However, the scope of the result of the translation is not the same
scope as that of the input. When the scope is extended when going under an
abstraction, the translation also adds a continuation argument. We can
establish the relationship between the input and output contexts using the
following inductive definition. Bindings introduced by abstractions use
\rref{tc-abs}, whereas other bindings use \rref{tc-up}.

\begin{definition}[\link{control/cps.v}{transCtx}{Context translation}]\ \\
\drules[tc]{$[[G ~ D ]]$}{Typing context relation}{nil,abs,up}
\end{definition}

We can now state our main lemma:

\begin{lemma}[\link{control/cps.v}{typing_transTm}{CPS translation is type preserving}]
Suppose $[[G ~ D ]]$.
\begin{enumerate}
\item If $[[ G ||- v : tau ]]$ then $[[ D ||- V{ v } : T{ tau } ]]$
\item If $[[ G ||- e : tau ]]$ and  $[[ D ||- w : C{ tau } ]]$ then 
  $[[ D ||- E{ e } w : Void ]]$
\item If $[[ ||- s : tau1 ~> tau ]]$ then $[[ ||- S{ s } : C{ tau } ]]$
\end{enumerate}
\end{lemma}

The proof of this lemma is by mutual induction on the typing derivation.  By
choosing a single variable as the initial continuation, we can observe how
this translation uses double negation on the input type. Alternatively, we can
use an initial continuation to get a complete program.

\begin{corollary}[\link{control/cps.v}{top_level1}{CPS translation is type preserving}]
If $[[ ||- e : tau ]]$ then $[[ ||- \x. E{e} x : (T{tau} -> Void) -> Void ]]$
and $[[ ||- E{e} (\y. exit y) : Void ]]$.
\end{corollary}

To show that the semantics is preserved by the simulation, we first need to show 
two substitution lemmas. The first shows that substitution commutes with the 
translation. Note that the ``continuation'' $[[w]]$ is part of the output scope, 
isn't translated like the term, value or stack. Therefore, we still need to translate
the value as we push it through the continuation.

\begin{lemma}[\link{control/cps.v}{transSubst3}{Substitution}]
\begin{enumerate}
\item $[[ (E{e} w)[V{v}/x] ]]$ = $[[ E{ e[v/x] } (w[V{v}/x]) ]]$
\item $[[ (V{v1})[V{v}/x] ]]$ = $[[ V{ v1[v/x] } ]]$
\item $[[ (S{s})[V{v}/x] ]]$ = $[[ S{ s[v/x] } ]]$
\end{enumerate}
\end{lemma}

The second lemma, called \emph{Kubstitution} by Pottier~\cite{pottier:cps},
lets us substitute for variables that are free in the continuation only, and
not in the expression.
\begin{lemma}[\link{control/cps.v}{transSubst2}{Kubstitution}]
\begin{enumerate}
\item 
  If $x \not\in [[fv(e)]]$ then $[[ (E{e} w)[v/x] ]]$ = $[[ E{ e[v/x] } (w[V{v}/x]) ]]$
\end{enumerate}
\end{lemma}

We can use these substitution lemmas to show that a step with the stack machine semantics
can be simulated by zero-or-more steps with the usual small-step semantics (augmented with 
a small-step rule for $\ottkw{exit}$. 

\begin{lemma}[\link{control/cps.v}{trans_step}{Semantic correctness}]
If $[[ <s1, e1> |-> <s2,e2>]]$ then $[[E{e1} S{s1} ~>* E{e2} S{s2}]]$
\end{lemma}


\section{Effect handlers}

The control operator $\ottkw{letcc}$ demonstrates how to \emph{reify} the
stack as a first class value.  However, in some ways this feature is both too
powerful and too inflexible.

On one hand, $\ottkw{letcc}$ can be inefficient to implement because saved
continuations can be used multiple times. This means that the programming
language's runtime needs to allocate memory for the saved stack and keep this
memory around until the stack reference goes out of scope and can be garbage
collected.  A more efficient version is \emph{one shot} continuations: saved
stacks that can be jumped to at most once.

At the same time, these continuations are not \emph{compose able}. The issue is
that $\ottkw{letcc}$ captures the \emph{entire} stack. But in some situations
it may be useful to only capture \emph{part} of the stack and compose that
part with the current stack in a new context.

OCaml 5's new feature, called \emph{effect handlers}, are based on
\emph{one-shot delimited continuations}. In this section, we will work through 
their support for \emph{delimited} continuations. However, for simplicity, we 
won't model the fact that they are single use here.

\subsection{Grammar and stack-based semantics}

Below, we introduce the grammar for this extension. Just like we used
$[[Nat]]$ as the type of exceptions (which means that we could tell what
exception was raised by looking at that number), we also represent effects
using natural numbers. When a user \emph{performs} an effect with
$[[perform v]]$, $[[v]]$ must be the number corresponding to an effect. Then
an effect handler is a frame on the stack that executes some code if that
particular effect has been performed.

The term $[[try e1 with x => e2 ; eff n y => e2']]$ installs an effect handler
during the execution of $[[e1]]$. If $[[e1]]$ returns a value, then the first 
branch $[[e2]]$ is taken, where $[[x]]$ is bound to that value. However, 
if $[[e1]]$ performs the effect $[[n]]$, then the second branch $[[e2']]$ is 
selected, and $[[y]]$ is bound to the continuation of the effect. This 
is the part of the stack that occurs between where the effect was performed 
up-to and including the effect handler itself. 

\begin{definition}[Grammar for effect handlers]
\[
\begin{array}{llcl}
\mathit{types}& [[tau]] &::=& [[DeCont tau1 tau2]]\ |\ [[Eff tau]] \\
\mathit{value}& [[v]] &::=& [[cont s]]\ |\ [[effect n]]\ \\
\mathit{term} & [[e]] &::=& [[perform v]]\ |\ [[try e1 with x => e2 ; eff n y => e2']]\ | \ [[continue v1 v2]] \\
\mathit{frame} & [[frame]] &::=& [[try _ with x => e2 ; eff n y => e2']] \\
\end{array}
\]
\end{definition}

\begin{definition}[Effect handler operational semantics] \ \\
\drules{$[[m |-> m']]$}{Stack-based small-step rules}
{ssm-handle,ssm-handle-ret,ssm-perform,ssm-continue}
\end{definition}

The \rref{ssm-handle} merely installs the effect handler by pushing the relevant 
information on the stack. This handler is popped in \rref{ssm-handle-ret}, where
 the returned value is passed to the first branch of the handler. 

As before, the value $[[cont s]]$ embeds a list of frames as value---what is
different this time is that when we perform an effect, we need to search the
current stack to find the handler for that effect, along the way saving all of
the stack frames that we encounter while searching for the effect.

The meta-operation $[[find_eff s n s']]$ searches stack $[[s]]$, looking for a handler 
for effect $[[n]]$, and accumulating frames in argument $[[s']]$. The \rref{ssm-perform}
calls this operation with an initial accumulator of the empty stack. 

\[
\begin{array}{l}
[[find_eff (frame : s) n s' ]] \\
\qquad = [[<s, e2'[cont (rev (frame : s'))/y]>]]\\
\qquad \texttt{when }[[frame]] = [[try _ with x => e2 ; eff n y => e2']] \\
\qquad = [[find_eff s n (frame:s')]] \\
\qquad \texttt{otherwise} \\
[[find_eff nil n s']] = [[< nil , perform (effect n) >]] \\
\end{array}
\]

When a matching frame is found, then the next machine stack is the one  
where the handler was installed $[[s]]$. The next machine term is the 
branch for that effect ($[[e2']]$). Furthermore, the variable $[[y]]$ is 
replaced with a continuation composed of the accumulated stack frames along with 
the handler. (By keeping the current handler, this semantics implements so 
called \emph{deep} effect handlers. Alternative semantics discard this frame.)
Because the accumulator is constructed while recurring down the stack, we 
must reverse the frames when constructing the continuation.

If the frame is not a handler for the current effect, then the operation calls
itself recursively, adding that frame to the accumulator. Finally, if there is
no handler for the effect, then the machine steps to a configuration with a
unhandled effect.

This saved continuation, which is available in the effect handler can be
invoked with the $[[continue v1 v2]]$ term. The operation of
$\ottkw{continue}$ is a little like that of $\ottkw{throw}$. However, instead
of discarding the current stack, this rule appends the saved stack to the
current stack with the notation $[[s1 ++ s2]]$. Stacks are represented as
lists, so this is merely a list append operation.

\subsection{Typing rules and type safety}

To type check programs that involve effect handlers, we need a more informative
type for our saved continuations. Our original typing judgment for stacks
only tracked the type of the term that was compatible with the frame at the
top of the stack. It did not say anything about the type of value that would
eventually be returned by the machine.  This is a feature of exceptions and
our previous control operators: we can prove type safety for this type system
without statically tracking this information. This makes sense because the
only thing that is done with the final returned value is to return it. No more
computation can happen because the stack is empty. 

In this setting, though, we need to know the bottom of the partial stack that
is reified by the effect handler because it will be composed with another
stack. We need to know that this composition will work out. If this type
information is not necessary (such as in checking a machine, or in the typing
rule for a non-delimited continuation) we can merely ignore the second type.

\begin{definition}[Delimited stack typing and values]\ \\
\drules[m]{$[[ ||- m ok ]]$}{Machine is well typed} 
{ok}
\drules[ts]{$[[||- s : tau1 ~> tau2]]$}{Stack typing}
{nil,cons}
\drules[tv]{$[[G ||- v : tau]]$}{Value typing}
{decont,eff}
\end{definition}

Therefore, our revised stack typing rule includes two types. These two types
are recorded in $[[DeCont tau1 tau2]]$, the type of a delimited continuation.

We also need to be careful about the typing of effect values. We assume that our 
type system is parameterized by some map $\Phi$, which maps effect numbers to 
their types. 

\begin{definition}[Typing rules for effect handlers]\ \\
\drules[te]{$[[G ||- e : tau]]$}{Term typing}
{perform,continue,handle}
\drules[tf]{$[[||- f : tau1 ~> tau2]]$}{Frame typing}
{handle}
\end{definition}

When we perform an effect, the type of the effect must match the context where
we perform that effect. The reason is that when we capture the continuation,
its type includes the context of the perform: the $[[tau1]]$ of
$[[DeCont tau1 tau2]]$ says that the sort of value expected by the captured
stack is $[[tau1]]$ --- i.e. the sort of value that we can put in this spot.
When we $\ottkw{continue}$ with this continuation, we must give it a value of 
the appropriate type. We also must only $\ottkw{continue}$ in a context 
that matches the bottom of the saved stack, as this stack will be composed with 
the one at this point in the execution. 

In \rref{te-handle}, the two branches must have the same type $[[tau]]$. In
the first case, the term acts like a $\ottkw{let}$, and continues with
$[[e2]]$ with $[[x]]$ as the result of $[[e1]]$. In the second case, the code
$[[e2']]$ executes in response to $[[e1]]$ performing an effect of type
$[[Eff (Phi n)]]$. That means that the saved delimited continuation will
accept a value of type $[[Phi n]]$, and produce a value of type
$[[tau]]$. Therefore $[[y]]$, which is bound to this saved continuation, must
have type $[[DeCont (Phi n) tau]]$.

The typing rule for the frame (i.e. effect handler on the stack) mirrors that
of the term. It records that it should be paired with an expression of type
$[[e1]]$ and produces a value of type $[[tau]]$.

Most of the preservation and progress proof from the previous section carries over. The most difficult part of setting this proof up is getting the definitions right. If we say, tried to use type $[[Cont tau]]$ instead of $[[DeCont tau1 tau2]]$ for the type of the saved stacks, we would not be able to show our results.

The key part of the preservation lemma is showing that we perform an effect, then the 
next machine state is well-formed.
\begin{lemma}[Effect finding]
If $[[ ||- s : tau1 ~> tau]]$ and $[[||- rev s1 : Phi n ~> tau1]]$ (for any $[[s1]]$), 
then $[[ ||- find_eff s n s1 ok ]]$
\end{lemma}
The proof of this lemma is by induction on the stack typing for $[[s]]$. Our accumulation stack, $[[s1]]$, needs to accept a term compatible with effect $[[n]]$ and produce a value 
that is compatible with the top of the stack $[[s]]$. 

When we perform an effect, the top of the stack must have type $[[Phi n]]$,
according to \rref{t-perform}. The accumulator starts empty, so its bottom
type must also be $[[Phi n]]$. The lemma ensures that starting from these
initial conditions, no matter where we find the effect handler we will have a
good machine state.

\section{Further reading}

The treatment of exceptions and control operations was inspired by PFPL
chapters 29 and 30. The material on effect handlers was inspired by Xavier
Leroy's course ``Control
Structures''~\url{https://xavierleroy.org/CdF/2023-2024/}. The draft book for
this course is available at \url{https://xavierleroy.org/control-structures/}.
More information about CPS conversion and its verification using Autosubst is
available in Pottier's paper~\cite{pottier:cps}.
