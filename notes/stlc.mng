\chapter[Type Safety for STLC]{Type Safety for a Simply-Typed Lambda Calculus}

This section gives a precise definition of the syntax of a simply-typed
lambda calculus, its type system and small-step operational semantics. For 
conciseness, we often refer to this language as STLC.

If you are new to programming language theory, this section also introduces
some of the mathematical concepts that we will be using throughout the
semester, such as inductively defined grammars, recursive definitions, and
proofs by structural induction.

STLC is actually a family of simple languages, with some freedom in the sorts
of features that are included. There must always be some sort of ``primitive''
type such as booleans, numbers, or even a unit type. And STLC always includes
first-class functions, i.e. $\lambda$-terms, making it a simplified version of
typed functional languages such as ML or Haskell. However, in other contexts
you may see it extended with various other features, such as records,
products, disjoint unions, variant types, etc.

\section{Syntax}

The syntax of the simply-typed lambda calculus is defined by a set of terms
and their associated set of types. By convention, we will use the metavariable
$[[e]]$ to refer to some arbitrary term and $[[tau]]$ to refer to some
arbitrary type. If you are familiar with algebraic datatypes, or inductive
datatypes, you can think of the following definitions along those lines.

\begin{definition}[Types]
The set of types is inductively defined by the following rules:
\begin{enumerate}
\item A base type, $[[Nat]]$, is a type.
\item If $[[tau1]]$ and $[[tau2]]$ are types, then $[[tau1 -> tau2]]$ is a type.
\end{enumerate}
The type $[[tau1 -> tau2]]$ represents the type of functions that take an argument of type $[[tau1]]$ and return a value of type $[[tau2]]$.
\end{definition}

\begin{definition}[Terms]
The set of terms is inductively defined by the following rules:
\begin{enumerate}
\item A natural number $k$ is a term.
\item A variable $x$ is a term. 
\item If $[[e]]$ is a term and $x$ is a variable, then $[[\x.e]]$ is a term (called a lambda abstraction).
The variable $x$ is the parameter and $e$ is the body is the body of the abstraction.
\item If $[[e1]]$ and $[[e2]]$ are terms, then $[[e1 e2]]$ is a term (called a function application).
\end{enumerate}
\end{definition}

The definition of terms refers to two other sets: natural numbers and
variables. The set of natural numbers, $[[nats]]$, are an infinite set of
numbers 0, 1, \ldots; we will use $i$, $j$ and $k$ to refer to arbitrary
natural numbers. We treat variables more abstractly. We assume that there is
some infinite set of variable \emph{names}, called $[[vars]]$, and that given
any finite set of variables, we can always find some variable that is not in
contained in that set. (We call such a variable \emph{fresh} because we
haven't used it yet.) If you like, you can think of names more concretely as
strings or numbers, but we won't allow all of the usual operations on strings
and numbers to be applied to names.

Now, the above definitions are a wordy way of describing an
inductively-defined grammar of abstract syntax trees.  In the future, we will
use a more concise notation, called Bakus-Naur form. For example, in BNF form,
we can provide a concise definition of the grammars for types and terms as
follows.

\begin{definition}[STLC Syntax (concise form)]
\[
\begin{array}{llcl}
\textit{numbers} & [[i]], [[j]], [[k]] & \in & \mathbb{N} \\
\textit{variables} & [[x]] & \in & [[vars]]  \\
\textit{types} & [[tau]] &::=& [[Nat]]\ |\ [[tau1 -> tau2]] \\
\textit{terms} & [[e]]   &::=& [[k]]\ |\ [[x]]\ |\ [[\x.e]]\ |\ [[e1 e2]] \\
\end{array}
\]
\end{definition}

\paragraph{Free variables}
Because types and terms are inductively defined sets, we can reason about them
using recursion and induction principles.  The recursion principle means that
we define recursive functions that takes terms or types as arguments and know
that the functions are total, as long as we call the functions over smaller
subterms.

For example, one function that we might define calculates the set of
\emph{free} variables in a term.

\begin{definition}[Free variables] 
We define the operation $[[fv(e)]]$, which calculates the set of variables
that occur \emph{free} in some term $[[e]]$, by structural recursion. 
\[
\begin{array}{lcll}
[[ fv(k)       ]] &=& [[ emptyset ]]  & \textit{ emptyset }  \\
[[ fv(x)       ]] &=& [[ { x } ]]  & \textit{ a singleton set } \\
[[ fv(e1 e2)   ]] &=& [[ fv(e1) U fv(e2) ]] & \textit{ union of sets } \\
[[ fv(\x . e)  ]] &=& [[ fv(e) - { x } ]] & \textit{ remove variable $x$ } \\
\end{array}
\]
\end{definition}
Each of the lines above describes the behavior of this function on the
different sorts of terms.  If the argument is a natural number constant $k$,
then it contains no free variables, so the result of the function is the
$[[emptyset]]$. Otherwise, if the argument is a single variable, then the
function returns a singleton set. If the argument is an application, then we
use recursion to find the free variables of each subterm and then combine
these sets using an ``union'' operation. Finally, in the last line of this
function, we find the free variables of the body of an abstraction, but then
remove the argument $x$ from that set because it does not appear free in
entire abstraction.

Variables that appear in terms that are not free are called \emph{bound}. For
example, in the term $[[\x. x y]]$, we have $x$ bound and $y$
free. Furthermore, some variables may occur in both bound and free positions
in terms; such as $x$ in the term $[[(\x. x y) x]]$.

\paragraph{Renaming}
Here is another example of a recursively defined function. Sometimes we would
like to change the names of free variables in terms.

A \emph{renaming}, $[[xi]]$, is a mapping from variables to variables. A
renaming has a \emph{domain}, $[[dom xi]]$ and a \emph{range} $[[rng xi]]$.
We use the notation $[[y/x]]$ for a single renaming that maps $[[x]]$ to
$[[y]]$, and the notation $[[y/x,xi]]$ to extend an existing renaming with a
new replacement for $[[x]]$.

\begin{definition}[Renaming application] We define the application of a
  renaming to a term, written with postfix notation $[[e<xi>]]$, as follows:
\[
\begin{array}{lcl}
[[ k< xi >      ]] &=& [[ k ]] \\
[[ x< xi >      ]] &=& [[ xi x ]] \\
[[ (e1 e2) < xi >  ]] &=& [[ (e1 <xi>) (e2 <xi>) ]] \\
[[ (\x . e) < xi > ]] &=& [[ \y . (e < y/x, xi >) ]] \mbox{ for $y$ not in $[[rng xi]]$} \\
\end{array}
\]
\end{definition}
We can only apply a renaming to a term when its domain includes 
the free variables defined in the term. In that case, our renaming
function is total: it produces an answer for any such term.

We have to be a bit careful in the last line of this definition. What if
$[[xi]]$ already maps the variable $[[x]]$ to some other variable? What if
$[[xi]]$ already maps some other variable to $[[x]]$? Our goal is to only
rename free variables: the function should leave the bound variables
alone. Inside the body of $[[\x.e]]$, the variable $[[x]]$ occurs bound, not
free. On the other hand, if we introduce a new $[[x]]$ through renaming an
existing free variable, we do not want it to be \emph{captured} by the
function. For example, if we rename $x$ to $y$, in the function $[[\x. y< x/y>
]]$, we do not want to produce $[[\x.x]]$.

Therefore, we pick some fresh variable $[[y]]$, and updating the renaming to
$[[(y/x,xi)]]$ in the recursive call. (If $[[x]]$ is already fresh, we can
keep using it.)  That way, we force the renaming that we use for the body of
the abstraction to not change the bounding structure of the term.

\paragraph{Substitution}
There is one final definition of a function defined by structural recursion
over terms: the application of a \emph{substitution} that applies to all free
variables in the term. 

A \emph{substitution}, $[[sigma]]$ is a mapping from variables to terms. As
above, it has a \emph{domain} (a set of variables) and a \emph{range} (this
time a set of terms). We use the notation $[[(e/x,sigma)]]$ to refer to the
substitution that maps variable $[[x]]$ to term $[[e]]$, but otherwise acts
like $[[sigma]]$.

As before, this definition only applies when the free variables of the term
are contained within the domain of the substitution. Furthermore, when
substituting in the body of an abstraction, we must be careful to avoid 
variable capture. 

\begin{definition}[Substitution application] We define the application of a
  substitution function to a term, written with postfix notation
  $[[ e[sigma] ]]$, as follows:
\[
\begin{array}{lcl}
[[ k [sigma]        ]] &=& [[ k ]] \\
[[ x [sigma]        ]] &=& [[ sigma x ]] \\
[[ (e1 e2) [sigma]  ]] &=& [[ (e1 [sigma] ) (e2 [sigma] ) ]] \\
[[ (\x . e) [sigma] ]] &=& [[ \y . (e [ y/x, sigma ]) ]] \ \mbox{when $y \not\in [[fv (rng sigma)]]$}\\
\end{array}
\]
\end{definition}


\paragraph{Variable binding, alpha-equivalence and all that} At this point, we
will start to be somewhat informal when it comes to bound variables in
terms. As you see above, we need to be careful about variable capture when
doing renaming and substitution. But we don't want to pollute our reasoning later
with these details.

Fortunately, we also don't want to distinguish between terms that differ only
in their use of bound variables, such as $[[\x.x]]$ and $[[\y.y]]$. There is a
relation called $\alpha$-equivalence that relates such terms, and from this
point forward we will say that our definitions are
``up-to-$\alpha$-equivalence''. What this means practically is that on one
hand, we must be sure that our definitions don't really depend on the names of
bound variables. In return, we can always assume that any bound variable is
distinct from any other variable, if we need it to be. This practice is called 
the ``Barendregt Variable Convention''\cite{barendregt:lambda-calculus}.

But, note that this is an informal convention, allowing us to follow the
common practice of describing lambda calculus terms as we have done above
(sometimes called using a named or nominal representation of variables). But
getting the details right is difficult (it requires maintaining careful
invariants about all definitions) and subtle. If you are working with a proof
assistant, you really do need to get the details right. In that context, it
also makes sense to use an approach (such as de Bruijn
indices~\cite{debruijn:nameless}) where the details are easier to get
right. This is what we will do in the accompanying mechanized proofs.

However, because using a named representation is standard practice, we will
continue to use that approach in these notes, glossing over details. This will
allow us to stay roughly equivalent to the proof scripts (which have other
details). Because of the informal nature of our discussion, there will be
minor omissions related to variable naming; but we won't stress about them.

\section{Type system}
Next we will define a typing relation for STLC. This relation has the form
$[[G |- e : tau]]$, which is read as ``in the typing context $[[G]]$, the term
$[[e]]$ has type $[[tau]]$.''  The typing context $[[G]]$, tells us what the
types of free variables should be. Therefore, we can view it as a finite map
from variables to types, and write it by listing all of the associations
$[[x:tau]]$.  If a term is in this relation we say that it ``type checks''.

We define the typing relation inductively, using the following rules. A term
type checks if we can find some tree that puts these rules together in a
\emph{derivation}. In each rule, the part below the line is the conclusion of
the rule, and the rule may have multiple premises. In a derivation tree, each
premise must be satisfied by subderivations, bottoming out with rules such as
\rref{t-var} or \rref{t-lit} that do not have any premises for the same
relation.

\begin{definition}[STLC type system]\ \\
\drules[t]{$[[G |- e : tau ]]$}{in context $[[G]]$, term $e$ has type $[[tau]]$}
{lit,var,abs,app}
\end{definition}

In the variable rule, we look up the type of the variable in the typing
context. This variable must have a definition in $[[G]]$ for this rule to be
used. If there is no type associated with $[[x]]$, then we say that the
variable is unbound and that the term fails to \emph{scope-check}.

In \rref{t-abs}, the rule for abstractions, we type check the body of the
function with a context that has been extended with a type for the bound
variable. The type of an abstraction is a function type $[[tau1->tau2]]$, that
states the required type of the parameter $[[tau1]]$ and the result type of
the body $[[tau2]]$.

\Rref{t-app}, which checks the application of functions, requires that the
argument to the function has the same type required by the function.

\section{Operational Semantics}

Is this type system meaningful? Our type system makes a distinction between
terms that type check (such as $[[(\x.x)3]]$) and terms that do not, such as
$[[(2 5)]]$. But how do we know that this distinction is useful? Do we have
the right rules?

The key property that we want is called \emph{type safety}. If a term type
checks, we should be able to evaluate it without triggering a certain class of
errors. 

One way to describe the evaluation of programs is through a \emph{small-step}
operational semantics. This is a mathematical definition of a relation between
a program $[[e]]$ and its value. We build up a small step semantics in two
parts. First, we define a single step relation, written $[[e ~> e']]$, to mean
that a term reduces to $[[e]]$ in one step.  Then we iterate this relation,
called the multistep relation and written $[[e ~>* e']]$, to talk about all of
the different programs that $[[e]]$ could reduce to after any number of steps,
including 0.

The multistep evaluations that we are interested in are the ones where we do
some number of small steps and get to an $[[e']]$ that has a very specific
form, a \emph{value}. If we have $[[e ~>* v]]$ then we say that \emph{$[[e]]$
evaluates to $[[v]]$}.

\begin{definition}[Value]
A \emph{value} is an expression that is either a natural number constant or an
abstraction.
\[  [[v]] ::= [[k]]\ |\ [[\x.e]] \]
\end{definition}

We define the single step relation inductively, using the inference rules below
that state when one term steps to another.

\begin{definition}[Small-step relation]\ \\
\drules[s]{$[[e ~> e']]$}{term $e$ steps to $[[e']]$}
{beta,app-congOne,app-congTwo}
\end{definition}

In each of these three rules, the part below the line says when the left term steps to the right term.
\Rref{step-beta} describe what happens when an abstraction is applied to an argument. In this case, 
we substitute the argument for the parameter in the 
body of the function. Note in this rule that the 
argument must be a value before substitution. If it
is not a value, then we cannot use this rule to take 
a step. This rule is the
key of a \emph{call-by-value} semantics. 

The second two rules each have premises that must be satisfied before they can be used. \Rref{step-app-congOne} applies when the function part of an application is not (yet) an abstraction. Similarly, the last rule applies when the argument part of an application is not (yet) a value. 

This small step relation is intended to be deterministic. Any term steps to at
most one new term.
\begin{lemma}[Determinism]
If $[[e ~> e1]]$ and $[[e ~> e2]]$ then $[[e1]]=[[e2]]$.
\end{lemma}

The small step relation is \emph{not} a function. For some terms $[[e]]$,
there is no term $[[e']]$ such that $[[e ~> e']]$. For example, if we have a
number in the function position, e.g. $[[(3 e)]]$, then the term does not step
and these terms do not evaluate to any value.

This is important. These terms are called \emph{stuck} and correspond to
crashing programs. For example, if we tried to use a number as function
pointer in the C language, then we might get a segmentation fault.

\section{Preservation and Progress}

Type safety is a crucial property of a typed programming language. It ensures
that a well-typed program will never ``go wrong'' during execution. For the
simply-typed lambda calculus, this means a program will not get stuck in a
state where it cannot take a reduction step but is not a final value.

The type safety proof is usually defined through two lemmas: Preservation and
Progress.

\paragraph{Preservation}

The \emph{preservation} lemma property states that if a term $[[e]]$ has type
$[[tau]]$, and it takes a single reduction step to $[[e']]$, then the new term
must also have the exact same type $[[tau]]$. In other words, the type is
``preserved'' through evaluation.

\begin{lemma}[Preservation]
If $[[ |- e : tau ]]$ and $[[e ~> e']]$ then $[[ |- e' : tau]]$.
\end{lemma}
Proof: The proof is by induction on the derivation of the reduction.
There are three cases, one for each of the rules 
that could have been used to conclude $[[e ~> e']]$. 
\begin{itemize}
\item In the case of \rref{s-beta}, we have that $[[e]]$ is of the form $[[(\x.e)v]]$ and $[[e']]$ is 
$[[ e[v/x] ]]$. We also know that the first term type checks, i.e.  $[[ |- (\x.e) v : tau ]]$. For this term to type check, we must have used \rref{t-app}, so we also know that $[[ |- (\x.e) : tau1 -> tau]]$ and 
$[[ |- v : tau1]]$. (This logical step is referred to 
as \emph{inversion} as we are reading a typing rule 
from bottom to top.). We can do this again, because the only way to make an abstraction to type check is 
rule \rref{t-abs}, so we must have also shown 
$[[ x : tau1 |- e : tau ]]$. At this point we, we will 
appeal to a \emph{substitution lemma} (see \ref{lem:single-subst} below) to 
finish this case of the proof. 
\item In the case of \rref{s-app-congOne}, we have the 
conclusion $[[e1 e2 ~> e1' e2]]$, and premise $[[e1 ~> e1']]$. For the first term to type check, we again must have also used \rref{t-app}, so we know that $[[|- e1 : tau1 -> tau]]$ and $[[|- e2 : tau1]]$. In this case we can use induction, because we know that $[[e1]]$, a term in the subderivation both steps and
type checks. So we know that $[[|- e1':tau1]]$. Now 
we can use \rref{t-app} to conclude that $[[|- e1' e2:tau]]$.
\item This case is similar to the one above.
\end{itemize}

In the \rref{s-beta} case, 
our proof above relies on this lemma, that we can 
write more formally:

\begin{corollary}[Single Substitution]
\label{lem:single-subst}
  If $[[ x:tau1 |- e : tau2 ]]$ and $[[ |- v : tau2 ]]$ then $[[ |- e [v/x] : tau1 ]]$
\end{corollary}

However, to prove this lemma, we must first generalize it. We cannot prove the
lemma directly as stated, because we need a version that gives us a stronger induction
hypothesis; one that works for any
substitution (not just a singleton one) and any context of terms (not just one
with a single variable assumption).

\begin{lemma}[Substitution]
  If $[[ G |- e : tau ]]$ and for all $[[x]] \in [[dom sigma]]$, we have
  $[[D |- sigma x : G x ]]$, then $[[D |- e[sigma] : tau ]]$.
\end{lemma}
Proof Sketch: The proof is by induction on the typing derivation.

\paragraph{Progress}

The second lemma, called \emph{progress} states that any well-typed term that
has not been completely reduced can always take at least one more reduction
step. It ensures that a well-typed term is not ``stuck.'' (i.e. is not a value
but cannot step).

\begin{lemma}[Progress]
If $[[ |- e : tau ]]$ then either $e$ is a value or there exists an $[[e']]$ such that 
$[[ e ~> e' ]]$.
\end{lemma}
We prove this lemma by induction in the typing derivation. In the rules where 
$[[e]]$ is already a value, then the proof is trivial.
Therefore we only need to consider when $[[e]]$ is an application of the 
form $[[e1 e2]]$, where $[[ |- e1 : tau1 -> tau]]$ and $[[ |- e2 : tau1 ]]$.
By induction on the first premise, we know that either $[[e1]]$ is a value or 
that it takes a step to some $[[e1']]$. If it takes a step, the entire 
application takes a step by \rref{s-app-cong1} and we are done.
Otherwise, if it is a value, then we know that 
it must be of the form $[[\x.e']]$, because it must have a function type.
By induction on the second premise, we know that either $[[e2]]$ is a value or 
that it takes a step to some $[[e2']]$. In the former case, the application 
steps to $[[e'[e2/x] ]]$ by \rref{s-beta}, in the latter case, the application 
steps to $[[(\x.e') e2']]$ by \rref{s-app-cong2}.


\section{What is type safety?}
We above claimed that type safety means that well-typed programs do not get
stuck. But what does this mean? Is that what we have really proven?  

There are languages and type systems that do not satisfy both of these lemmas,
yet we still might like to say that they are type safe. Can we come up with a
more general definition? Something that is implied by preservation/progress
but doesn't itself require them to be true.

Perhaps we would like to prove something like below, where the multistep
relation $[[~>*]]$ is iteration of the single-step relation any number of
times.  If a closed term type checks then it must evaluate to a value with the
same type.
\begin{conjecture}[Terminating Type Safety]
If $[[|- e : tau]]$ then there exists some value $v$ such that $[[e ~>* v]]$
and $[[|- v : tau]]$.
\end{conjecture}

This conjecture seems straightforward to prove from progress and
preservation. By progress we know that either a term is a value or that it
steps. By preservation, we know that it if it steps, it has the same type. But
what we are missing from a straightforward proof is the fact that this
conjecture says that evaluation \emph{terminates}. How do we know that we will
eventually reach a value in some finite number of steps?

It turns out that this conjecture is true, but we are not yet ready to prove
it directly. But even though the conjecture is true, it is not a good
definition of type safety: even though all well-typed STLC programs halt, that
is not true of most programming languages. And we would like to have a
definition of type safety that also applies to those languages. One that shows that 
well-typed programs do not get stuck, while not requiring them to produce values.

There are two solutions to this issue.

\paragraph{A coinductive definition}

Now consider the following \emph{coinductive} definition.
\begin{definition}[Runs safely]
A program $[[e]]$ \emph{runs safely}, if it is a value or 
if $[[e ~> e']]$, and $[[e']]$ \emph{runs safely}. 
\end{definition}

This is exactly the definition we want to use in a type safety theorem.
\begin{theorem}[Type Safety]
If $[[|- e : tau]]$ then $[[e]]$ \emph{runs safely}.
\end{theorem}

Just as in an inductive definitions, the definition of ``runs safely'' refers
to itself. But we are interpreting this definition coinductively, so it
includes both finite an infinite runs. In other words, if a program steps to
another program, which steps to another program, and so on, infinitely, then
it is included in this relation.

Coinductive definitions come with \emph{coinduction} principles. We usually
use induction principles to show that some property holds about an element of
an inductive definition that we already have. As we ``consume'' this
definition, we can assume, by induction, that the property is true for the
subterms of the definition. For example, when proving the preservation lemma,
we assumed that the lemma held for the subterms of the evaluation derivation.

The principle of coinduction applies when we want to ``generate'' an element of 
a coinductive definition. Watch!

We will prove type safety through coinduction. Given a well typed term 
$[[|- e : tau]]$, the progress lemma tells us that it is either a value or that
it steps. If it is a value, then we know directly that it runs safely. If it steps, i.e. 
if we have $[[e ~> e']]$, then by preservation, we know that $[[|- e' : tau]]$.
By the principle of coinduction, we know that $[[e']]$ runs safely. So we can 
conclude that $[[e]]$ runs safely. 

When are we allowed to use a coinductive hypothesis? With induction, we were
limited to ``consuming'' subterms or smaller derivations. But when we use a
coinductive hypothesis, it cannot be the last step of the proof. We need to
do something with the result of this hypothesis to generate our coinductive 
definition.

This can be a bit confusing at first, and I encourage you to look at proofs 
completed with coinduction in the first place to get the hang of using this 
principle.

\paragraph{An inductive definition}

Alternatively, if you are still uncomfortable with coinduction, we can define 
what it means to run safely another way. 

We say that an expression $[[e]]$ steps to $[[e']]$ in $[[k]]$ steps 
using the following inductive definition.
\begin{definition}
\drules[ms-k]{$[[e ~k> e' ]]$}{$k$ steps}
{refl,step}
\end{definition}

\begin{definition}[Safe for $k$]
An expression evaluates safely for $k$ steps if it either there is some
$[[e']]$, such that $[[e ~k> e']]$, or there is some number of steps $j$
strictly less than $k$ where the term terminates with a value 
(i.e. there is some $v$ and $j <= k$ such that $[[e ~j> v]]$).
\end{definition}

We can now state type safety using this step-counting definition. We can't
really talk about an infinite computation, but we can know that for an
arbitrarily long time, $e$ will run safely during that time.

\begin{theorem}
If $[[|- e : tau]]$ then for all natural numbers $k$, $e$ is safe for $k$.
\end{theorem}
We show this result by induction on $k$.
If $k$ is 0, then the result is trivial. All expressions run safely for zero steps. 
If $k$ is nonzero, then progress states that $[[e]]$ is either a value or steps. 
If it is a value, we are also done, as values are safe for any $k$.
If it steps to some $[[e']]$, then preservation tells us that $[[|- e' : tau]]$.
By induction, we know that $[[e']]$ is safe for $k-1$. So either $[[e' ~j> v]]$, i.e. $[[e']]$ steps to some value $[[v]]$ within $j$ steps, for some $j < k-1$, or $[[e' ~k-1> e'']]$.
In the first case, we have $[[e ~j+1> v]]$ which is a safe evaluation for $[[e]]$. In the 
second case, we have $[[e ~k> e'']]$, which is also a safe evaluation for $[[e]]$.

\section{Further reading}
The type safety proof for the simply-typed lambda calculus is explained in a
number of textbooks including TAPL~\cite{pierce:tapl}, PFPL~\cite{harper:pfpl}
and Software Foundations~\cite{Pierce:SF2}.  Each of these sources defines
type safety as the conjunction of preservation and progress.

Milner~\cite{milner:polymorphism} proved a \emph{type soundness} theorem,
which states that well-typed ML programs cannot ``go wrong''. To do so, he
constructed a denotational semantics of the ML language that maps every ML
program to either some mathematical value (like a number or continuous
function), to a special element indicating divergence ($\bot$), or to a
special element called ``wrong'' that indicates a run-time error. He then
proved that if a program type checks, then its denotation does not include the
``wrong'' element.

Wright and Felleisen~\cite{wright:syntactic} observed that run-time errors
could be ruled out by using a small-step operational semantics. They defined
syntactic type soundness as showing preservation (inspired by subject
reduction from combinatory logic), characterizing ``stuck'' or ``faulty''
expressions, and then showing that faulty expressions are not typeable
(i.e. progress). They put these together with a strong soundness theorem that
says that well-typed programs either diverge or reduce to values of the
appropriate type.

